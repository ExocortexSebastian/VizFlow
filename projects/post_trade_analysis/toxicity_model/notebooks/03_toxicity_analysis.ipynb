{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# 03 - Toxicity Analysis\n\nTag trades with conditions and analyze which conditions lead to poor forward returns.\n\n**Toxicity Conditions:**\n- **Adverse Selection**: Opponent's first-level size too large -> we may be picked off\n- **Information Leakage**: Opponent's first-level size too small -> informed trader hiding\n- **Wide Spread**: Large bid-ask spread indicates uncertainty\n- **Touch Imbalance**: Extreme imbalance in top-of-book\n\n**Goal**: Identify conditions where our alpha predictions are unreliable.\n\n**VizFlow v0.5.0 Note:**\nZero prices return `null` instead of `inf` for y_* columns."
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VizFlow version: 0.5.0\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import vizflow as vf\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"VizFlow version: {vf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "import sys\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"configs\"))\n",
    "from default import config\n",
    "\n",
    "vf.set_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Load Data with Forward Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "DATE = \"11110101\"  # Use same date for trade and alpha\n\n# Load and prepare trade data\ndf_trade = vf.scan_trade(DATE)\ndf_trade = vf.parse_time(df_trade, timestamp_col=\"alpha_ts\")\ndf_trade = df_trade.with_columns([\n    ((pl.col(\"bid_px0\") + pl.col(\"ask_px0\")) / 2).alias(\"mid\"),\n    (pl.col(\"ask_px0\") - pl.col(\"bid_px0\")).alias(\"spread\"),\n    # Spread in bps\n    ((pl.col(\"ask_px0\") - pl.col(\"bid_px0\")) / ((pl.col(\"bid_px0\") + pl.col(\"ask_px0\")) / 2) * 10000).alias(\"spread_bps\"),\n])\n\n# Load and prepare alpha data (same date)\ndf_alpha = vf.scan_alpha(DATE)\ndf_alpha = vf.parse_time(df_alpha, timestamp_col=\"ticktime\")\ndf_alpha = df_alpha.with_columns(\n    ((pl.col(\"bid_px0\") + pl.col(\"ask_px0\")) / 2).alias(\"mid\")\n)\n\n# Calculate forward returns\nHORIZONS = [60, 180, 1800]  # 60s, 3m, 30m\ndf = vf.forward_return(\n    df_trade,\n    df_alpha,\n    horizons=HORIZONS,\n    trade_time_col=\"elapsed_alpha_ts\",\n    alpha_time_col=\"elapsed_ticktime\",\n    price_col=\"mid\",\n    symbol_col=\"ukey\",\n)\n\ndf = df.collect()\nprint(f\"Total trades: {len(df):,}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Define Toxicity Conditions\n",
    "\n",
    "Tag trades based on market conditions at execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition metrics added\n",
      "shape: (9, 4)\n",
      "┌────────────┬────────────┬─────────────────┬───────────────┐\n",
      "│ statistic  ┆ spread_bps ┆ touch_imbalance ┆ opponent_size │\n",
      "│ ---        ┆ ---        ┆ ---             ┆ ---           │\n",
      "│ str        ┆ f64        ┆ f64             ┆ f64           │\n",
      "╞════════════╪════════════╪═════════════════╪═══════════════╡\n",
      "│ count      ┆ 318.0      ┆ 318.0           ┆ 318.0         │\n",
      "│ null_count ┆ 0.0        ┆ 0.0             ┆ 0.0           │\n",
      "│ mean       ┆ 17.104034  ┆ 0.012557        ┆ 25851.36478   │\n",
      "│ std        ┆ 10.212137  ┆ 0.425407        ┆ 14165.221155  │\n",
      "│ min        ┆ 4.414037   ┆ -0.920789       ┆ 1077.0        │\n",
      "│ 25%        ┆ 8.920607   ┆ -0.272756       ┆ 14565.0       │\n",
      "│ 50%        ┆ 13.44086   ┆ 0.004429        ┆ 25701.0       │\n",
      "│ 75%        ┆ 27.434842  ┆ 0.307745        ┆ 38292.0       │\n",
      "│ max        ┆ 42.342978  ┆ 0.908481        ┆ 49973.0       │\n",
      "└────────────┴────────────┴─────────────────┴───────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Calculate condition metrics\n",
    "# Column names from presets: bid_size0, ask_size0 (not bid_qty0/ask_qty0)\n",
    "\n",
    "df = df.with_columns([\n",
    "    # Top-of-book imbalance: (bid_size - ask_size) / (bid_size + ask_size)\n",
    "    # Positive = more on bid side, Negative = more on ask side\n",
    "    (\n",
    "        (pl.col(\"bid_size0\") - pl.col(\"ask_size0\")) / \n",
    "        (pl.col(\"bid_size0\") + pl.col(\"ask_size0\"))\n",
    "    ).alias(\"touch_imbalance\"),\n",
    "    \n",
    "    # Opponent size (opposite side of our trade)\n",
    "    pl.when(pl.col(\"order_side\") == \"Buy\")\n",
    "        .then(pl.col(\"ask_size0\"))\n",
    "        .otherwise(pl.col(\"bid_size0\"))\n",
    "        .alias(\"opponent_size\"),\n",
    "])\n",
    "\n",
    "print(\"Condition metrics added\")\n",
    "print(df.select([\"spread_bps\", \"touch_imbalance\", \"opponent_size\"]).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity condition rates:\n",
      "  tox_wide_spread: 93.71%\n",
      "  tox_adverse_selection: 0.00%\n",
      "  tox_info_leakage: 0.00%\n",
      "  tox_imbalance: 11.95%\n"
     ]
    }
   ],
   "source": [
    "# Define thresholds for toxicity conditions\n",
    "# NOTE: These are example thresholds - calibrate based on your data\n",
    "\n",
    "SPREAD_HIGH_BPS = 5.0       # Wide spread threshold (bps)\n",
    "OPPONENT_LARGE = 50000      # Large opponent size threshold\n",
    "OPPONENT_SMALL = 1000       # Small opponent size threshold\n",
    "IMBALANCE_EXTREME = 0.7     # Extreme imbalance threshold (abs value)\n",
    "\n",
    "# Tag trades with toxicity conditions\n",
    "df = df.with_columns([\n",
    "    # Wide spread - high uncertainty\n",
    "    (pl.col(\"spread_bps\") > SPREAD_HIGH_BPS).alias(\"tox_wide_spread\"),\n",
    "    \n",
    "    # Large opponent - potential adverse selection\n",
    "    (pl.col(\"opponent_size\") > OPPONENT_LARGE).alias(\"tox_adverse_selection\"),\n",
    "    \n",
    "    # Small opponent - potential information leakage (informed trader hiding)\n",
    "    (pl.col(\"opponent_size\") < OPPONENT_SMALL).alias(\"tox_info_leakage\"),\n",
    "    \n",
    "    # Extreme touch imbalance\n",
    "    (pl.col(\"touch_imbalance\").abs() > IMBALANCE_EXTREME).alias(\"tox_imbalance\"),\n",
    "])\n",
    "\n",
    "# Summary of toxicity flags\n",
    "tox_cols = [c for c in df.columns if c.startswith(\"tox_\")]\n",
    "print(\"Toxicity condition rates:\")\n",
    "for col in tox_cols:\n",
    "    rate = df[col].sum() / len(df) * 100\n",
    "    print(f\"  {col}: {rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Analyze Forward Returns by Toxicity\n",
    "\n",
    "Compare forward returns for toxic vs non-toxic trades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "\n",
      "tox_wide_spread:\n",
      "shape: (2, 5)\n",
      "┌─────────────────┬───────┬─────────────┬────────────┬───────────────┐\n",
      "│ tox_wide_spread ┆ count ┆ mean_return ┆ std_return ┆ median_return │\n",
      "│ ---             ┆ ---   ┆ ---         ┆ ---        ┆ ---           │\n",
      "│ bool            ┆ u32   ┆ f64         ┆ f64        ┆ f64           │\n",
      "╞═════════════════╪═══════╪═════════════╪════════════╪═══════════════╡\n",
      "│ false           ┆ 20    ┆ -0.027414   ┆ 0.046791   ┆ -0.022366     │\n",
      "│ true            ┆ 298   ┆ -0.020648   ┆ 0.025823   ┆ -0.01899      │\n",
      "└─────────────────┴───────┴─────────────┴────────────┴───────────────┘\n",
      "----------------------------------------\n",
      "\n",
      "tox_adverse_selection:\n",
      "shape: (1, 5)\n",
      "┌───────────────────────┬───────┬─────────────┬────────────┬───────────────┐\n",
      "│ tox_adverse_selection ┆ count ┆ mean_return ┆ std_return ┆ median_return │\n",
      "│ ---                   ┆ ---   ┆ ---         ┆ ---        ┆ ---           │\n",
      "│ bool                  ┆ u32   ┆ f64         ┆ f64        ┆ f64           │\n",
      "╞═══════════════════════╪═══════╪═════════════╪════════════╪═══════════════╡\n",
      "│ false                 ┆ 318   ┆ -0.020882   ┆ 0.026382   ┆ -0.019165     │\n",
      "└───────────────────────┴───────┴─────────────┴────────────┴───────────────┘\n",
      "----------------------------------------\n",
      "\n",
      "tox_info_leakage:\n",
      "shape: (1, 5)\n",
      "┌──────────────────┬───────┬─────────────┬────────────┬───────────────┐\n",
      "│ tox_info_leakage ┆ count ┆ mean_return ┆ std_return ┆ median_return │\n",
      "│ ---              ┆ ---   ┆ ---         ┆ ---        ┆ ---           │\n",
      "│ bool             ┆ u32   ┆ f64         ┆ f64        ┆ f64           │\n",
      "╞══════════════════╪═══════╪═════════════╪════════════╪═══════════════╡\n",
      "│ false            ┆ 318   ┆ -0.020882   ┆ 0.026382   ┆ -0.019165     │\n",
      "└──────────────────┴───────┴─────────────┴────────────┴───────────────┘\n",
      "----------------------------------------\n",
      "\n",
      "tox_imbalance:\n",
      "shape: (2, 5)\n",
      "┌───────────────┬───────┬─────────────┬────────────┬───────────────┐\n",
      "│ tox_imbalance ┆ count ┆ mean_return ┆ std_return ┆ median_return │\n",
      "│ ---           ┆ ---   ┆ ---         ┆ ---        ┆ ---           │\n",
      "│ bool          ┆ u32   ┆ f64         ┆ f64        ┆ f64           │\n",
      "╞═══════════════╪═══════╪═════════════╪════════════╪═══════════════╡\n",
      "│ false         ┆ 280   ┆ -0.019462   ┆ 0.024425   ┆ -0.018724     │\n",
      "│ true          ┆ 38    ┆ -0.029751   ┆ 0.036452   ┆ -0.036009     │\n",
      "└───────────────┴───────┴─────────────┴────────────┴───────────────┘\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def analyze_toxicity(df: pl.DataFrame, tox_col: str, return_col: str = \"y_60s\"):\n",
    "    \"\"\"Analyze forward returns for toxic vs non-toxic trades.\"\"\"\n",
    "    result = df.group_by(tox_col).agg([\n",
    "        pl.len().alias(\"count\"),\n",
    "        pl.col(return_col).mean().alias(\"mean_return\"),\n",
    "        pl.col(return_col).std().alias(\"std_return\"),\n",
    "        pl.col(return_col).median().alias(\"median_return\"),\n",
    "    ]).sort(tox_col)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Analyze each toxicity condition\n",
    "print(\"=\" * 60)\n",
    "for tox_col in tox_cols:\n",
    "    print(f\"\\n{tox_col}:\")\n",
    "    result = analyze_toxicity(df, tox_col, \"y_60s\")\n",
    "    print(result)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward return comparison (mean) by toxicity condition:\n",
      "======================================================================\n",
      "\n",
      "tox_wide_spread:\n",
      "shape: (2, 5)\n",
      "┌─────────────────┬─────┬─────────────┬─────────────┬─────────────┐\n",
      "│ tox_wide_spread ┆ n   ┆ y_60s_bps   ┆ y_3m_bps    ┆ y_30m_bps   │\n",
      "│ ---             ┆ --- ┆ ---         ┆ ---         ┆ ---         │\n",
      "│ bool            ┆ u32 ┆ f64         ┆ f64         ┆ f64         │\n",
      "╞═════════════════╪═════╪═════════════╪═════════════╪═════════════╡\n",
      "│ false           ┆ 20  ┆ -274.143543 ┆ -288.526057 ┆ -436.144039 │\n",
      "│ true            ┆ 298 ┆ -206.482201 ┆ -197.728974 ┆ -184.866335 │\n",
      "└─────────────────┴─────┴─────────────┴─────────────┴─────────────┘\n",
      "\n",
      "tox_adverse_selection:\n",
      "shape: (1, 5)\n",
      "┌───────────────────────┬─────┬─────────────┬─────────────┬────────────┐\n",
      "│ tox_adverse_selection ┆ n   ┆ y_60s_bps   ┆ y_3m_bps    ┆ y_30m_bps  │\n",
      "│ ---                   ┆ --- ┆ ---         ┆ ---         ┆ ---        │\n",
      "│ bool                  ┆ u32 ┆ f64         ┆ f64         ┆ f64        │\n",
      "╞═══════════════════════╪═════╪═════════════╪═════════════╪════════════╡\n",
      "│ false                 ┆ 318 ┆ -208.815351 ┆ -200.971727 ┆ -196.83194 │\n",
      "└───────────────────────┴─────┴─────────────┴─────────────┴────────────┘\n",
      "\n",
      "tox_info_leakage:\n",
      "shape: (1, 5)\n",
      "┌──────────────────┬─────┬─────────────┬─────────────┬────────────┐\n",
      "│ tox_info_leakage ┆ n   ┆ y_60s_bps   ┆ y_3m_bps    ┆ y_30m_bps  │\n",
      "│ ---              ┆ --- ┆ ---         ┆ ---         ┆ ---        │\n",
      "│ bool             ┆ u32 ┆ f64         ┆ f64         ┆ f64        │\n",
      "╞══════════════════╪═════╪═════════════╪═════════════╪════════════╡\n",
      "│ false            ┆ 318 ┆ -208.815351 ┆ -200.971727 ┆ -196.83194 │\n",
      "└──────────────────┴─────┴─────────────┴─────────────┴────────────┘\n",
      "\n",
      "tox_imbalance:\n",
      "shape: (2, 5)\n",
      "┌───────────────┬─────┬─────────────┬────────────┬─────────────┐\n",
      "│ tox_imbalance ┆ n   ┆ y_60s_bps   ┆ y_3m_bps   ┆ y_30m_bps   │\n",
      "│ ---           ┆ --- ┆ ---         ┆ ---        ┆ ---         │\n",
      "│ bool          ┆ u32 ┆ f64         ┆ f64        ┆ f64         │\n",
      "╞═══════════════╪═════╪═════════════╪════════════╪═════════════╡\n",
      "│ false         ┆ 280 ┆ -194.624013 ┆ -210.66282 ┆ -195.04171  │\n",
      "│ true          ┆ 38  ┆ -297.511212 ┆ 60.687795  ┆ -232.636548 │\n",
      "└───────────────┴─────┴─────────────┴────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Analyze across multiple horizons\n",
    "print(\"Forward return comparison (mean) by toxicity condition:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for tox_col in tox_cols:\n",
    "    print(f\"\\n{tox_col}:\")\n",
    "    result = df.group_by(tox_col).agg([\n",
    "        pl.len().alias(\"n\"),\n",
    "        (pl.col(\"y_60s\").mean() * 10000).alias(\"y_60s_bps\"),\n",
    "        (pl.col(\"y_3m\").mean() * 10000).alias(\"y_3m_bps\"),\n",
    "        (pl.col(\"y_30m\").mean() * 10000).alias(\"y_30m_bps\"),\n",
    "    ]).sort(tox_col)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Combined Toxicity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward returns by toxicity score (0 = clean, 4 = most toxic):\n",
      "shape: (3, 5)\n",
      "┌───────────┬─────┬─────────────┬─────────────┬─────────────┐\n",
      "│ tox_score ┆ n   ┆ y_60s_bps   ┆ y_3m_bps    ┆ y_30m_bps   │\n",
      "│ ---       ┆ --- ┆ ---         ┆ ---         ┆ ---         │\n",
      "│ i32       ┆ u32 ┆ f64         ┆ f64         ┆ f64         │\n",
      "╞═══════════╪═════╪═════════════╪═════════════╪═════════════╡\n",
      "│ 0         ┆ 18  ┆ -274.143543 ┆ -288.526057 ┆ -436.144039 │\n",
      "│ 1         ┆ 264 ┆ -191.310699 ┆ -207.66808  ┆ -182.352114 │\n",
      "│ 2         ┆ 36  ┆ -297.511212 ┆ 60.687795   ┆ -232.636548 │\n",
      "└───────────┴─────┴─────────────┴─────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Count number of toxicity flags per trade\n",
    "df = df.with_columns(\n",
    "    (pl.col(\"tox_wide_spread\").cast(pl.Int32)\n",
    "     + pl.col(\"tox_adverse_selection\").cast(pl.Int32)\n",
    "     + pl.col(\"tox_info_leakage\").cast(pl.Int32)\n",
    "     + pl.col(\"tox_imbalance\").cast(pl.Int32)\n",
    "    ).alias(\"tox_score\")\n",
    ")\n",
    "\n",
    "# Analyze by toxicity score\n",
    "print(\"Forward returns by toxicity score (0 = clean, 4 = most toxic):\")\n",
    "result = df.group_by(\"tox_score\").agg([\n",
    "    pl.len().alias(\"n\"),\n",
    "    (pl.col(\"y_60s\").mean() * 10000).alias(\"y_60s_bps\"),\n",
    "    (pl.col(\"y_3m\").mean() * 10000).alias(\"y_3m_bps\"),\n",
    "    (pl.col(\"y_30m\").mean() * 10000).alias(\"y_30m_bps\"),\n",
    "]).sort(\"tox_score\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Toxic vs Clean trades:\n",
      "shape: (2, 6)\n",
      "┌──────────┬─────┬───────────┬─────────────┬─────────────┬─────────────┐\n",
      "│ is_toxic ┆ n   ┆ pct       ┆ y_60s_bps   ┆ y_3m_bps    ┆ y_30m_bps   │\n",
      "│ ---      ┆ --- ┆ ---       ┆ ---         ┆ ---         ┆ ---         │\n",
      "│ bool     ┆ u32 ┆ f64       ┆ f64         ┆ f64         ┆ f64         │\n",
      "╞══════════╪═════╪═══════════╪═════════════╪═════════════╪═════════════╡\n",
      "│ false    ┆ 18  ┆ 5.660377  ┆ -274.143543 ┆ -288.526057 ┆ -436.144039 │\n",
      "│ true     ┆ 300 ┆ 94.339623 ┆ -206.482201 ┆ -197.728974 ┆ -184.866335 │\n",
      "└──────────┴─────┴───────────┴─────────────┴─────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Binary: toxic vs clean\n",
    "df = df.with_columns(\n",
    "    (pl.col(\"tox_score\") > 0).alias(\"is_toxic\")\n",
    ")\n",
    "\n",
    "print(\"\\nToxic vs Clean trades:\")\n",
    "result = df.group_by(\"is_toxic\").agg([\n",
    "    pl.len().alias(\"n\"),\n",
    "    (pl.len() / len(df) * 100).alias(\"pct\"),\n",
    "    (pl.col(\"y_60s\").mean() * 10000).alias(\"y_60s_bps\"),\n",
    "    (pl.col(\"y_3m\").mean() * 10000).alias(\"y_3m_bps\"),\n",
    "    (pl.col(\"y_30m\").mean() * 10000).alias(\"y_30m_bps\"),\n",
    "]).sort(\"is_toxic\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 5. Toxicity by Order Side\n",
    "\n",
    "Check if toxicity affects buys and sells differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity analysis by order side:\n",
      "shape: (4, 5)\n",
      "┌────────────┬──────────┬─────┬─────────────┬─────────────┐\n",
      "│ order_side ┆ is_toxic ┆ n   ┆ y_60s_bps   ┆ y_3m_bps    │\n",
      "│ ---        ┆ ---      ┆ --- ┆ ---         ┆ ---         │\n",
      "│ str        ┆ bool     ┆ u32 ┆ f64         ┆ f64         │\n",
      "╞════════════╪══════════╪═════╪═════════════╪═════════════╡\n",
      "│ Buy        ┆ false    ┆ 9   ┆ 166.481687  ┆ null        │\n",
      "│ Buy        ┆ true     ┆ 128 ┆ -167.10705  ┆ -224.086756 │\n",
      "│ Sell       ┆ false    ┆ 9   ┆ -494.456158 ┆ -288.526057 │\n",
      "│ Sell       ┆ true     ┆ 172 ┆ -240.607331 ┆ -182.224396 │\n",
      "└────────────┴──────────┴─────┴─────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(\"Toxicity analysis by order side:\")\n",
    "result = df.group_by([\"order_side\", \"is_toxic\"]).agg([\n",
    "    pl.len().alias(\"n\"),\n",
    "    (pl.col(\"y_60s\").mean() * 10000).alias(\"y_60s_bps\"),\n",
    "    (pl.col(\"y_3m\").mean() * 10000).alias(\"y_3m_bps\"),\n",
    "]).sort([\"order_side\", \"is_toxic\"])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TOXICITY ANALYSIS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Total trades: 318\n",
      "Clean trades: 18 (5.7%)\n",
      "Toxic trades: 300 (94.3%)\n",
      "\n",
      "Condition breakdown:\n",
      "  tox_wide_spread: 298 (93.7%)\n",
      "  tox_adverse_selection: 0 (0.0%)\n",
      "  tox_info_leakage: 0 (0.0%)\n",
      "  tox_imbalance: 38 (11.9%)\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics\n",
    "total = len(df)\n",
    "toxic_count = df[\"is_toxic\"].sum()\n",
    "clean_count = total - toxic_count\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TOXICITY ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal trades: {total:,}\")\n",
    "print(f\"Clean trades: {clean_count:,} ({clean_count/total*100:.1f}%)\")\n",
    "print(f\"Toxic trades: {toxic_count:,} ({toxic_count/total*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nCondition breakdown:\")\n",
    "for col in tox_cols:\n",
    "    count = df[col].sum()\n",
    "    print(f\"  {col}: {count:,} ({count/total*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Based on findings:\n",
    "\n",
    "1. **Threshold Calibration**: Adjust thresholds based on your data distribution\n",
    "2. **Feature Engineering**: Add more toxicity conditions if needed\n",
    "3. **Model Integration**: Use toxicity flags to filter unreliable predictions\n",
    "4. **VizFlow Enhancement**: If patterns are useful, add `vf.tag_condition()` to VizFlow\n",
    "\n",
    "**Record findings in FEEDBACK.md** for VizFlow improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c80816",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}